{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fd46510",
   "metadata": {},
   "source": [
    "#Q1\n",
    "\n",
    "The Fast R-CNN consists of a CNN (usually pre-trained on the ImageNet classification task) with its final pooling layer replaced by an “ROI pooling” layer and its final FC layer is replaced by two branches — a (K + 1) category softmax layer branch and a category-specific bounding box regression branch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909aba7a",
   "metadata": {},
   "source": [
    "#Q2\n",
    "\n",
    "Faster R-CNN is a single-stage model that is trained end-to-end. It uses a novel region proposal network (RPN) for generating region proposals, which save time compared to traditional algorithms like Selective Search. It uses the ROI Pooling layer to extract a fixed-length feature vector from each region proposal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d52e82",
   "metadata": {},
   "source": [
    "#Q3\n",
    "\n",
    "It cannot be implemented real time as it takes around 47 seconds for each test image. The selective search algorithm is a fixed algorithm. Therefore, no learning is happening at that stage. This could lead to the generation of bad candidate region proposals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b2fcab",
   "metadata": {},
   "source": [
    "#Q4\n",
    "\n",
    "A Region Proposal Network, or RPN, is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc34ddca",
   "metadata": {},
   "source": [
    "#Q5\n",
    "\n",
    "ROI pooling solves the problem of fixed image size requirement for object detection network. ROI pooling produces the fixed-size feature maps from non-uniform inputs by doing max-pooling on the inputs. The number of output channels is equal to the number of input channels for this layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9464b228",
   "metadata": {},
   "source": [
    "#Q6\n",
    "\n",
    "Fully Convolutional Networks, or FCNs, are an architecture used mainly for semantic segmentation. They employ solely locally connected layers, such as convolution, pooling and upsampling. Avoiding the use of dense layers means less parameters (making the networks faster to train)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a127ce",
   "metadata": {},
   "source": [
    "#Q7\n",
    "\n",
    "Anchor boxes are a set of predefined bounding boxes of a certain height and width. These boxes are defined to capture the scale and aspect ratio of specific object classes you want to detect and are typically chosen based on object sizes in your training datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdf4d18",
   "metadata": {},
   "source": [
    "#Q8\n",
    "\n",
    "SSD has two components: a backbone model and SSD head. Backbone model usually is a pre-trained image classification network as a feature extractor. This is typically a network like ResNet trained on ImageNet from which the final fully connected classification layer has been removed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9442e53",
   "metadata": {},
   "source": [
    "#Q9\n",
    "\n",
    "SSD uses a matching phase while training, to match the appropriate anchor box with the bounding boxes of each ground truth object within an image. Essentially, the anchor box with the highest degree of overlap with an object is responsible for predicting that object's class and its location."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e7d995",
   "metadata": {},
   "source": [
    "#Q10\n",
    "\n",
    "The MSCNN consists of a proposal sub-network and a detection sub-network. In the proposal sub-network, detection is performed at multiple output layers, so that receptive fields match objects of different scales. These complementary scale-specific detectors are combined to produce a strong multi-scale object detector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935006a7",
   "metadata": {},
   "source": [
    "#Q11\n",
    "\n",
    " atrous convolution is akin to the standard convolution except that the weights of an atrous convolution kernel are spaced r locations apart, i.e., the kernel of dilated convolution layers are sparse. Fig. 3: Standard vs Dilated Kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae7d6fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

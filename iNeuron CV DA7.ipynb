{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f57a35ef",
   "metadata": {},
   "source": [
    "#Q1\n",
    "\n",
    "Covariate shift occurs when the distribution of variables in the training data is different to real-world or testing data. This means that the model may make the wrong predictions once it is deployed, and its accuracy will be significantly lower."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea44e567",
   "metadata": {},
   "source": [
    "#Q2\n",
    "\n",
    "Batch-Normalization (BN) is an algorithmic method which makes the training of Deep Neural Networks (DNN) faster and more stable. It consists of normalizing activation vectors from hidden layers using the first and the second statistical moments (mean and variance) of the current batch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f2fa4e",
   "metadata": {},
   "source": [
    "#Q3\n",
    "\n",
    "The LeNet architecture is an excellent “first architecture” for Convolutional Neural Networks (especially when trained on the MNIST dataset, an image dataset for handwritten digit recognition). LeNet is small and easy to understand — yet large enough to provide interesting results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e7b07e",
   "metadata": {},
   "source": [
    "#Q4\n",
    "\n",
    "The Alexnet has eight layers with learnable parameters. The model consists of five layers with a combination of max pooling followed by 3 fully connected layers and they use Relu activation in each of these layers except the output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32fefef",
   "metadata": {},
   "source": [
    "#Q5\n",
    "\n",
    "When there are more layers in the network, the value of the product of derivative decreases until at some point the partial derivative of the loss function approaches a value close to zero, and the partial derivative vanishes. We call this the vanishing gradient problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7702f16d",
   "metadata": {},
   "source": [
    "#Q6\n",
    "\n",
    "In biological brain functionality, there is a concept called lateral inhibition. This refers to the capacity of one stimulated neuron to bring its neighbors under control. The main agenda for us is to have a local peak value for finding the maximum value in the neighborhood."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35811be8",
   "metadata": {},
   "source": [
    "#Q7\n",
    "\n",
    "The regularization used in this network is L2 with a weight decay of 5e-4. It was trained on GTX580 GPU which contains 3GB of memory. It has an error rate of 16.4 in the ImageNet Large Scale Visual Recognition Challenge(ILSVRC)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2560230a",
   "metadata": {},
   "source": [
    "#Q8\n",
    "\n",
    "VGG- Network is a convolutional neural network model proposed by K. Simonyan and A. Zisserman in the paper “Very Deep Convolutional Networks for Large-Scale Image Recognition”. This architecture achieved top-5 test accuracy of 92.7% in ImageNet, which has over 14 million images belonging to 1000 classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11667d3",
   "metadata": {},
   "source": [
    "#Q10\n",
    "\n",
    "The simplest way to reduce overfitting is to increase the data, and this technique helps in doing so. Data augmentation is a regularization technique, which is used generally when we have images as data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c957856",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

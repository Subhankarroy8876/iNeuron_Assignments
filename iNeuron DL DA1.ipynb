{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e3d188b",
   "metadata": {},
   "source": [
    "#Q1\n",
    "\n",
    "summation, in physiology, the additive effect of several electrical impulses on a neuromuscular junction, the junction between a nerve cell and a muscle cell. Individually the stimuli cannot evoke a response, but collectively they can generate a response.\n",
    "A threshold activation function (or simply the activation function, also known as squashing function) results in an output signal only when an input signal exceeding a specific threshold value comes as an input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3b0113",
   "metadata": {},
   "source": [
    "#Q2\n",
    "\n",
    "In Mathematics, a step function (also called as staircase function) is defined as a piecewise constant function, that has only a finite number of pieces. In other words, a function on the real numbers can be described as a finite linear combination of indicator functions of given intervals.\n",
    "A step function is a function like that used by the original Perceptron. The output is a certain value, A1, if the input sum is above a certain threshold and A0 if the input sum is below a certain threshold. The values used by the Perceptron were A1 = 1 and A0 = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666a3d9e",
   "metadata": {},
   "source": [
    "#Q3\n",
    "\n",
    "The McCulloch-Pitts model was an extremely simple artificial neuron. The inputs could be either a zero or a one. And the output was a zero or a one. And each input could be either excitatory or inhibitory. Now the whole point was to sum the inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06631ba",
   "metadata": {},
   "source": [
    "#Q4\n",
    "\n",
    "MADALINE. MADALINE (Many ADALINE) is a three-layer (input, hidden, output), fully connected, feed-forward artificial neural network architecture for classification that uses ADALINE units in its hidden and output layers, i.e. its activation function is the sign function. The three-layer network uses memistors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582f7854",
   "metadata": {},
   "source": [
    "#Q5\n",
    "\n",
    "one of the challenges with the single-layer perceptrons is that the constraints limit the algorithm's ability to accurately classify data. Specifically, the primary constraint is that the data must be linearly separable.\n",
    "Perceptrons only represent linearly separable problems. They fail to converge if the training examples are not linearly separable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400e7a35",
   "metadata": {},
   "source": [
    "#Q6\n",
    "\n",
    "Clearly not all decision problems are linearly separable: they cannot be solved using a linear decision boundary. Problems like these are termed linearly inseparable.\n",
    "    In neural networks, a hidden layer is located between the input and output of the algorithm, in which the function applies weights to the inputs and directs them through an activation function as the output. In short, the hidden layers perform nonlinear transformations of the inputs entered into the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86850306",
   "metadata": {},
   "source": [
    "#Q7\n",
    "\n",
    "The XOR problem is that we need to build a Neural Network (a perceptron in our case) to produce the truth table related to the XOr logical operator. This is a binary classification problem. Hence, supervised learning is a better way to solve it. In this case, we will be using perceptrons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bc01e9",
   "metadata": {},
   "source": [
    "#Q9\n",
    "\n",
    "The output layer is formed when different weights are applied to input nodes and the cumulative effect per node is taken. After this, the neurons collectively give the output layer to compute the output signals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2996a0c",
   "metadata": {},
   "source": [
    "#Q10\n",
    "\n",
    "ANN architecture is based on the structure and function of the biological neural network. Similar to neurons in the brain, ANN also consists of neurons which are arranged in various layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5732f0bb",
   "metadata": {},
   "source": [
    "#Q11\n",
    "\n",
    "Backpropagation Algorithm:\n",
    "\n",
    "Step 1: Inputs X, arrive through the preconnected path. Step 2: The input is modeled using true weights W. Weights are usually chosen randomly. Step 3: Calculate the output of each neuron from the input layer to the hidden layer to the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9f5773",
   "metadata": {},
   "source": [
    "#Q12\n",
    "\n",
    "Here are some advantages of Artificial Neural Networks ( ANN)\n",
    "\n",
    "Storing information on the entire network: Information such as in traditional programming is stored on the entire network, not on a database. The disappearance of a few pieces of information in one place does not restrict the network from functioning. \n",
    "\n",
    "The ability to work with inadequate knowledge: After ANN training, the data may produce output even with incomplete information. The lack of performance here depends on the importance of the missing information. \n",
    "\n",
    "It has fault tolerance:  Corruption of one or more cells of ANN does not prevent it from generating output. This feature makes the networks fault-tolerant. \n",
    "\n",
    "\n",
    "Disadvantages of Artificial Neural Networks (ANN)\n",
    "\n",
    "Hardware dependence:  Artificial neural networks require processors with parallel processing power, by their structure. For this reason, the realization of the equipment is dependent. \n",
    "\n",
    "Unexplained functioning of the network: This is the most important problem of ANN. When ANN gives a probing solution, it does not give a clue as to why and how. This reduces trust in the network. \n",
    "\n",
    "Assurance of proper network structure:  There is no specific rule for determining the structure of artificial neural networks. The appropriate network structure is achieved through experience and trial and error. \n",
    "\n",
    "The difficulty of showing the problem to the network:  ANNs can work with numerical information. Problems have to be translated into numerical values before being introduced to ANN. The display mechanism to be determined here will directly influence the performance of the network. This depends on the user's ability. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129ecab1",
   "metadata": {},
   "source": [
    "#Q13\n",
    "\n",
    "a. Gradient descent is an optimization algorithm which is commonly-used to train machine learning models and neural networks.  Training data helps these models learn over time, and the cost function within gradient descent specifically acts as a barometer, gauging its accuracy with each iteration of parameter updates. Until the function is close to or equal to zero, the model will continue to adjust its parameters to yield the smallest possible error. Once machine learning models are optimized for accuracy, they can be powerful tools for artificial intelligence (AI) and computer science applications. \n",
    "\n",
    "b. A recurrent neural network (RNN) is a class of artificial neural networks where connections between nodes can create a cycle, allowing output from some nodes to affect subsequent input to the same nodes. This allows it to exhibit temporal dynamic behavior. Derived from feedforward neural networks, RNNs can use their internal state (memory) to process variable length sequences of inputs. This makes them applicable to tasks such as unsegmented, connected handwriting recognition or speech recognition. Recurrent neural networks are theoretically Turing complete and can run arbitrary programs to process arbitrary sequences of inputs.\n",
    "\n",
    "The term \"recurrent neural network\" is used to refer to the class of networks with an infinite impulse response, whereas \"convolutional neural network\" refers to the class of finite impulse response. Both classes of networks exhibit temporal dynamic behavior. A finite impulse recurrent network is a directed acyclic graph that can be unrolled and replaced with a strictly feedforward neural network, while an infinite impulse recurrent network is a directed cyclic graph that can not be unrolled.\n",
    "\n",
    "Both finite impulse and infinite impulse recurrent networks can have additional stored states, and the storage can be under direct control by the neural network. The storage can also be replaced by another network or graph if that incorporates time delays or has feedback loops. Such controlled states are referred to as gated state or gated memory, and are part of long short-term memory networks (LSTMs) and gated recurrent units. This is also called Feedback Neural Network (FNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe7fa21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

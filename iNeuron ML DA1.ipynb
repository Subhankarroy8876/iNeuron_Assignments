{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7685f1e",
   "metadata": {},
   "source": [
    "#Q1\n",
    "\n",
    "Machine learning is a way for computer programs to improve their performance on a task over time given more data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8a651c",
   "metadata": {},
   "source": [
    "#Q2\n",
    "\n",
    "Machine learning algorithms have had good results on problems such has spam detection in email, cancer diagnosis, fraudulent credit card transactions, and automatically driving vehicles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee17c7c",
   "metadata": {},
   "source": [
    "#Q3\n",
    "\n",
    "A labeled training set is a collection of data where one of the features of the data indicates the class the training example belongs to. A labeled training set is used in supervised learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea3ff6f",
   "metadata": {},
   "source": [
    "#Q4\n",
    "\n",
    "The two most common supervised learning tasks are regression and classification. In a regression problem we our prediciton is a scalar value. When we're trying to solve a classification problem, our output is either 1 or 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319b2ab7",
   "metadata": {},
   "source": [
    "#Q5\n",
    "\n",
    "Common unsupervised tasks include clustering, visualization, dimensionality reduction and association rule learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178a933b",
   "metadata": {},
   "source": [
    "#Q6\n",
    "\n",
    "I would use a reinforcement learning approach. Reinforcement learning is a system where an \"agent\" observes the environment, selects and performs actions, then recieves a reward or punishment based on the result of the action. Over time the agent learns by itself what is the most productive strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed41538",
   "metadata": {},
   "source": [
    "#Q7\n",
    "\n",
    "I would use some sort of clustering algorithm that can find the decision boundaries in the groups automatically. This is an unsupervised approach. However, if I already knew the categories of my customers, then I would choose a supervised approach and go with a classification algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa597f16",
   "metadata": {},
   "source": [
    "#Q8\n",
    "\n",
    "I would frame it as a supervised learning problem because humans have a general idea about what spam is and what it isn't. We can use this notion to create a labeled dataset for an algorithm to learn from."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c94810",
   "metadata": {},
   "source": [
    "#Q9\n",
    "\n",
    "An online learning system learns from new data on-the-fly. As a result, the system is trained incrementally either by using one example at a time or using a mini-batch approach. This keeps each learning step cheap and memory efficient.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abcc371",
   "metadata": {},
   "source": [
    "#Q10\n",
    "\n",
    "Out-of-core learning is used when a dataset is too large to fit into a computer's memory. The algorithm loads part of the data, runs a training step, then repeats the process until it has run on all the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48491e8",
   "metadata": {},
   "source": [
    "#Q11\n",
    "\n",
    "Instance-based learning algorithms use a measure of similarity to generalize to new cases. In an instance-based learning system, the algorithm learns the examples by heart, then uses the similarity measure to generalize."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbce4eb",
   "metadata": {},
   "source": [
    "#Q12\n",
    "\n",
    "A hyperparameter is a parameter of the learning algorithm, not the model. For example, in a simple linear regression problem our model is parameterized by theta which is a vector of weights. In order to find the best values for theta we have a cost function which is run repeatedly by the gradient descent algorithm. Gradient descent has a hyperparameter called alpha which is the learning rate of the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86afd2fd",
   "metadata": {},
   "source": [
    "#Q13\n",
    "\n",
    "The goal for a model-based algorithm is to be able to generalize to new examples. To do this, model based algorithms search for optimal values for the model's parameters, often called theta. This searching, or \"learning\", is what machine learning is all about. Model-based system learn by minimizing a cost function that measures how bad the system is at making predicitons on new data, plus a penalty for model complexity if the model is regularized. To make a prediction, a new instance's features are fed into a hypothesis function which uses the minimized theta found by repeatedly running the cost function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f314a5d2",
   "metadata": {},
   "source": [
    "#Q14\n",
    "\n",
    "1.Not gathering enough data, or sampling noise. Sampling noise means we'll have non-representative data as a result of chance.\n",
    "\n",
    "2.Using a dataset that is not representative of the cases you want to generalize to. This is called sampling bias. For example, if you want to train an algorithm with \"cat videos\", and all your videos are from YouTube, you're actually training an algorithm to learn about \"YouTube cat videos.\"\n",
    "\n",
    "3.Your dataset is full of missing values, outliers, and noise (poor measurments).\n",
    "\n",
    "4.The features in your dataset are irrelevant. Garbage in, garbage out.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f768694c",
   "metadata": {},
   "source": [
    "#Q15\n",
    "\n",
    "This is a case where the model is overfitting the training data. To couteract overfitting, we can reduce the complexity of the model by removing features or constraining the parameters. We could gather more data. Finally we can reduce noisiness in the data by fixing errors and removing outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c2458b",
   "metadata": {},
   "source": [
    "#Q16\n",
    "\n",
    "When we want to know how well our model generalizes to new cases we prefer to use a test set instead of actually deploying the system. To build the test set we split the training data (50-50, 60-40, 80-20 are common splits) into a training set and test set. Our model is training with the training set. Then we use the model to run predictions on the test set. Our error rate on the test set is called the generalization error or out-of-sample error. This error tells us how well our model performs on examples it has never seen before.\n",
    "\n",
    "If the training error is low, but the generalization error is high, it means we're overfitting our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2314cd1",
   "metadata": {},
   "source": [
    "#Q17\n",
    "\n",
    "Let's say we have a linear model and we want to perform some hyperparameter tuning to reduce the generalization error. One way to do this 100 different models with 100 different hyperparameter values using the training set and finding the generalization error with the test set. You find the best hyperparameter value gives you 5% generalization error.\n",
    "\n",
    "So you launch the model into production and find you're seeing 15% generalization error. This isn't going as expected. What happened?\n",
    "\n",
    "The problem is that for each iteration of hyperparameter tuning, you measured the generalization error then updated the model using the same test set. In other words, your produced the best generalization error for the test set. The test set no longer represents cases the model hasn't seen before.\n",
    "\n",
    "A common solution to this problem is to have a second holdout set called the validation set. You train multiple models with various hyperparameters using the training set, you select the model and hyperparameters that perform best on the validation set, and when you are happy about your model you run a single final test against the test set to get an estimate of the generalization error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f0abde",
   "metadata": {},
   "source": [
    "#Q18\n",
    "\n",
    "Your model will not be generalizable to new examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1a6953",
   "metadata": {},
   "source": [
    "#Q19\n",
    "\n",
    "Cross-validation helps us compare models without wasting too much training data in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3dce96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

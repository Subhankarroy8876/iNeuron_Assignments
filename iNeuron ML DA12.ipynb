{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b192c92c",
   "metadata": {},
   "source": [
    "#Q1\n",
    "\n",
    "Prior probability shows the likelihood of an outcome in a given dataset. For example, in the mortgage case, P(Y) is the default rate on a home mortgage, which is 2%. P(Y|X) is called the conditional probability, which provides the probability of an outcome given the evidence, that is, when the value of X is known."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cd4ee3",
   "metadata": {},
   "source": [
    "#Q2\n",
    "\n",
    "Posterior probability is a revised probability that takes into account new available information. For example, let there be two urns, urn A having 5 black balls and 10 red balls and urn B having 10 black balls and 5 red balls. Now if an urn is selected at random, the probability that urn A is chosen is 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164c68e2",
   "metadata": {},
   "source": [
    "#Q3\n",
    "\n",
    "The likelihood term, P(Y|X) is the probability of getting a result for a given value of the parameters. It is what you label probability. The posterior and prior terms are what you describe as likelihoods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae53e27",
   "metadata": {},
   "source": [
    "#Q4\n",
    "\n",
    "It is a classification technique based on Bayes' Theorem with an assumption of independence among predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature.\n",
    "Naive Bayes is called naive because it assumes that each input variable is independent. This is a strong assumption and unrealistic for real data; however, the technique is very effective on a large range of complex problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6766386d",
   "metadata": {},
   "source": [
    "#Q5\n",
    "\n",
    "Bayes Optimal Classifier is a probabilistic model that finds the most probable prediction using the training data and space of hypotheses to make a prediction for a new data instance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea0e229",
   "metadata": {},
   "source": [
    "#Q6\n",
    "\n",
    "Features of Bayesian learning methods:\n",
    "– This provides a more flexible approach to learning than algorithms that completely eliminate a hypothesis if it is found to be inconsistent with any single example.\n",
    "– a probability distribution over observed data for each possible hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab5c3e4",
   "metadata": {},
   "source": [
    "#Q7\n",
    "\n",
    "Consistent Learners. • A learner L using a hypothesis H and training data D is said to be a consistent learner if it always outputs a hypothesis with zero error on D whenever H contains such a hypothesis. • By definition, a consistent learner must produce a hypothesis in the version space for H given D."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4719f03",
   "metadata": {},
   "source": [
    "#Q8\n",
    "\n",
    "It is simple and easy to implement.\n",
    "It doesn't require as much training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140946a7",
   "metadata": {},
   "source": [
    "#Q9\n",
    "\n",
    "If your test data set has a categorical variable of a category that wasn't present in the training data set, the Naive Bayes model will assign it zero probability and won't be able to make any predictions in this regard. \n",
    "This algorithm is also notorious as a lousy estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521968aa",
   "metadata": {},
   "source": [
    "#Q10\n",
    "\n",
    "a. Since a Naive Bayes text classifier is based on the Bayes's Theorem, which helps us compute the conditional probabilities of occurrence of two events based on the probabilities of occurrence of each individual event, encoding those probabilities is extremely useful.\n",
    "b. Naive Bayes classifiers work by correlating the use of tokens (typically words, or sometimes other things), with spam and non-spam e-mails and then using Bayes' theorem to calculate a probability that an email is or is not spam.\n",
    "c.  The Naive Bayes algorithm is a supervised machine learning algorithm based on the Bayes' theorem. It is a probabilistic classifier that is often used in NLP tasks like sentiment analysis (identifying a text corpus' emotional or sentimental tone or opinion)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816e91fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

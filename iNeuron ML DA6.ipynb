{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "377e2d0b",
   "metadata": {},
   "source": [
    "#Q1\n",
    "\n",
    "Training a model simply means learning (determining) good values for all the weights and the bias from labeled examples. In supervised learning, a machine learning algorithm builds a model by examining many examples and attempting to find a model that minimizes loss; this process is called empirical risk minimization.\n",
    "3 steps to training a machine learning model\n",
    "Step 1: Begin with existing data. Machine learning requires us to have existing data—not the data our application will use when we run it, but data to learn from. \n",
    "Step 2: Analyze data to identify patterns. \n",
    "Step 3: Make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0666b1c2",
   "metadata": {},
   "source": [
    "#Q2\n",
    "\n",
    "The No Free Lunch Theorem is often thrown around in the field of optimization and machine learning, often with little understanding of what it means or implies.\n",
    "\n",
    "The theorem states that all optimization algorithms perform equally well when their performance is averaged across all possible problems.\n",
    "\n",
    "It implies that there is no single best optimization algorithm. Because of the close relationship between optimization, search, and machine learning, it also implies that there is no single best machine learning algorithm for predictive modeling problems such as classification and regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5ad0d2",
   "metadata": {},
   "source": [
    "#Q3\n",
    "\n",
    "Cross-validation is a resampling procedure used to evaluate machine learning models on a limited data sample.\n",
    "\n",
    "The procedure has a single parameter called k that refers to the number of groups that a given data sample is to be split into. As such, the procedure is often called k-fold cross-validation. When a specific value for k is chosen, it may be used in place of k in the reference to the model, such as k=10 becoming 10-fold cross-validation.\n",
    "\n",
    "Cross-validation is primarily used in applied machine learning to estimate the skill of a machine learning model on unseen data. That is, to use a limited sample in order to estimate how the model is expected to perform in general when used to make predictions on data not used during the training of the model.\n",
    "\n",
    "It is a popular method because it is simple to understand and because it generally results in a less biased or less optimistic estimate of the model skill than other methods, such as a simple train/test split.\n",
    "\n",
    "The general procedure is as follows:\n",
    "\n",
    "Shuffle the dataset randomly.\n",
    "Split the dataset into k groups\n",
    "For each unique group:\n",
    "Take the group as a hold out or test data set\n",
    "Take the remaining groups as a training data set\n",
    "Fit a model on the training set and evaluate it on the test set\n",
    "Retain the evaluation score and discard the model\n",
    "Summarize the skill of the model using the sample of model evaluation scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eaba788",
   "metadata": {},
   "source": [
    "#Q4\n",
    "\n",
    "The bootstrap method is a statistical technique for estimating quantities about a population by averaging estimates from multiple small data samples. Importantly, samples are constructed by drawing observations from a large data sample one at a time and returning them to the data sample after they have been chosen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4427806c",
   "metadata": {},
   "source": [
    "#Q5\n",
    "\n",
    "It basically tells you how much better your classifier is performing over the performance of a classifier that simply guesses at random according to the frequency of each class. Cohen's kappa is always less than or equal to 1. Values of 0 or less, indicate that the classifier is useless\n",
    "If kappa coefficient equals to 1, then the classified image and the ground truth image are totally identical. So, the higher the kappa coefficient, the more accurate the classification is. Apart from the overall accuracy, the accuracy of class identification needs to be assessed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6413ba3d",
   "metadata": {},
   "source": [
    "#Q6\n",
    "\n",
    "Ensemble methods is a machine learning technique that combines several base models in order to produce one optimal predictive model. To better understand this definition lets take a step back into ultimate goal of machine learning and model building.\n",
    "The goal of any machine learning problem is to find a single model that will best predict our wanted outcome. Rather than making one model and hoping this model is the best/most accurate predictor we can make, ensemble methods take a myriad of models into account, and average those models to produce one final model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdfc54d",
   "metadata": {},
   "source": [
    "#Q7\n",
    "\n",
    "A descriptive model describes a system or other entity and its relationship to its environment. It is generally used to help specify and/or understand what the system is, what it does, and how it does it. A geometric model or spatial model is a descriptive model that represents geometric and/or spatial relationships.\n",
    "\n",
    "1. Traffic and Engagement Reports\n",
    "One example of descriptive analytics is reporting. If your organization tracks engagement in the form of social media analytics or web traffic, you’re already using descriptive analytics.\n",
    "\n",
    "These reports are created by taking raw data—generated when users interact with your website, advertisements, or social media content—and using it to compare current metrics to historical metrics and visualize trends.\n",
    "\n",
    "2. Financial Statement Analysis\n",
    "Another example of descriptive analytics that may be familiar to you is financial statement analysis. Financial statements are periodic reports that detail financial information about a business and, together, give a holistic view of a company’s financial health.\n",
    "\n",
    "3. Demand Trends\n",
    "Descriptive analytics can also be used to identify trends in customer preference and behavior and make assumptions about the demand for specific products or services.\n",
    "\n",
    "Streaming provider Netflix’s trend identification provides an excellent use case for descriptive analytics. Netflix’s team—which has a track record of being heavily data-driven—gathers data on users’ in-platform behavior. They analyze this data to determine which TV series and movies are trending at any given time and list trending titles in a section of the platform’s home screen.\n",
    "\n",
    "4. Aggregated Survey Results\n",
    "Descriptive analytics is also useful in market research. When it comes time to glean insights from survey and focus group data, descriptive analytics can help identify relationships between variables and trends.\n",
    "\n",
    "For instance, you may conduct a survey and identify that as respondents’ age increases, so does their likelihood to purchase your product. If you’ve conducted this survey multiple times over several years, descriptive analytics can tell you if this age-purchase correlation has always existed or if it was something that only occurred this year."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb915fa",
   "metadata": {},
   "source": [
    "#Q8\n",
    "\n",
    "There are 3 main metrics for model evaluation in regression:\n",
    "1. R Square/Adjusted R Square\n",
    "\n",
    "2. Mean Square Error(MSE)/Root Mean Square Error(RMSE)\n",
    "\n",
    "3. Mean Absolute Error(MAE)\n",
    "\n",
    "R Square/Adjusted R Square\n",
    "R Square measures how much variability in dependent variable can be explained by the model. It is the square of the Correlation Coefficient(R) and that is why it is called R Square.\n",
    "\n",
    "Mean Square Error(MSE)/Root Mean Square Error(RMSE)\n",
    "While R Square is a relative measure of how well the model fits dependent variables, Mean Square Error is an absolute measure of the goodness for the fit.\n",
    "\n",
    "Mean Absolute Error(MAE)\n",
    "Mean Absolute Error(MAE) is similar to Mean Square Error(MSE). However, instead of the sum of square of error in MSE, MAE is taking the sum of the absolute value of error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfc7b38",
   "metadata": {},
   "source": [
    "#Q9\n",
    "\n",
    "i) Descriptive Analytics, which use data aggregation and data mining to provide insight into the past and answer: “What has happened?”\n",
    "\n",
    "Predictive Analytics, which use statistical models and forecasts techniques to understand the future and answer: “What could happen?”\n",
    "\n",
    "\n",
    "    \n",
    "ii) Underfitting is a situation when your model is too simple for your data. More formally, your hypothesis about data distribution is wrong and too simple — for example, your data is quadratic and your model is linear. This situation is also called high bias. This means that your algorithm can do accurate predictions, but the initial assumption about the data is incorrect.\n",
    "    Opposite, overfitting is a situation when your model is too complex for your data. More formally, your hypothesis about data distribution is wrong and too complex — for example, your data is linear and your model is high-degree polynomial. This situation is also called high variance. This means that your algorithm can’t do accurate predictions — changing the input data only a little, the model output changes very much.\n",
    "\n",
    "\n",
    "    \n",
    "iii) Bootstrapping is any test or metric that relies on random sampling with replacement.It is a method that helps in many situations like validation of a predictive model performance, ensemble methods, estimation of bias and variance of the parameter of a model etc. It works by performing sampling with replacement from the original dataset, and at the same time assuming that the data points that have not been choses are the test dataset. We can repeat this procedure several times and compute the average score as estimation of our model performance. Also, Bootstrapping is related to the ensemble training methods, because we can build a model using each bootstrap datasets and “bag” these models in an ensemble using the majority voting (for classification) or computing the average (for numerical predictions) for all of these models as our final result.\n",
    "\n",
    "Cross validation is a procedure for validating a model's performance, and it is done by splitting the training data into k parts. We assume that the k-1 parts is the training set and use the other part is our test set. We can repeat that k times differently holding out a different part of the data every time. Finally, we take the average of the k scores as our performance estimation. Cross validation can suffer from bias or variance. Increasing the number of splits, the variance will increase too and the bias will decrease. On the other hand, if we decrease the number of splits, the bias will increase and the variance will decrease.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8051825",
   "metadata": {},
   "source": [
    "#Q10\n",
    "\n",
    "i) LOOCV for Evaluating Machine Learning Algorithms\n",
    "by Jason Brownlee on July 27, 2020 in Python Machine Learning\n",
    "Tweet Tweet  Share\n",
    "Last Updated on August 26, 2020\n",
    "\n",
    "The Leave-One-Out Cross-Validation, or LOOCV, procedure is used to estimate the performance of machine learning algorithms when they are used to make predictions on data not used to train the model.\n",
    "\n",
    "It is a computationally expensive procedure to perform, although it results in a reliable and unbiased estimate of model performance. Although simple to use and no configuration to specify, there are times when the procedure should not be used, such as when you have a very large dataset or a computationally expensive model to evaluate.\n",
    "\n",
    "In this tutorial, you will discover how to evaluate machine learning models using leave-one-out cross-validation.\n",
    "\n",
    "After completing this tutorial, you will know:\n",
    "\n",
    "The leave-one-out cross-validation procedure is appropriate when you have a small dataset or when an accurate estimate of model performance is more important than the computational cost of the method.\n",
    "How to use the scikit-learn machine learning library to perform the leave-one-out cross-validation procedure.\n",
    "How to evaluate machine learning algorithms for classification and regression using leave-one-out cross-validation.\n",
    "Kick-start your project with my new book Machine Learning Mastery With Python, including step-by-step tutorials and the Python source code files for all examples.\n",
    "\n",
    "Let’s get started.\n",
    "\n",
    "LOOCV for Evaluating Machine Learning Algorithms\n",
    "LOOCV for Evaluating Machine Learning Algorithms\n",
    "Photo by Heather Harvey, some rights reserved.\n",
    "\n",
    "Tutorial Overview\n",
    "This tutorial is divided into three parts; they are:\n",
    "\n",
    "LOOCV Model Evaluation\n",
    "LOOCV Procedure in Scikit-Learn\n",
    "LOOCV to Evaluate Machine Learning Models\n",
    "LOOCV for Classification\n",
    "LOOCV for Regression\n",
    "LOOCV Model Evaluation\n",
    "Cross-validation, or k-fold cross-validation, is a procedure used to estimate the performance of a machine learning algorithm when making predictions on data not used during the training of the model.\n",
    "\n",
    "The cross-validation has a single hyperparameter “k” that controls the number of subsets that a dataset is split into. Once split, each subset is given the opportunity to be used as a test set while all other subsets together are used as a training dataset.\n",
    "\n",
    "This means that k-fold cross-validation involves fitting and evaluating k models. This, in turn, provides k estimates of a model’s performance on the dataset, which can be reported using summary statistics such as the mean and standard deviation. This score can then be used to compare and ultimately select a model and configuration to use as the “final model” for a dataset.\n",
    "\n",
    "Typical values for k are k=3, k=5, and k=10, with 10 representing the most common value. This is because, given extensive testing, 10-fold cross-validation provides a good balance of low computational cost and low bias in the estimate of model performance as compared to other k values and a single train-test split.\n",
    "\n",
    "ii) In statistical analysis of binary classification, the F-score or F-measure is a measure of a test's accuracy. It is calculated from the precision and recall of the test, where the precision is the number of true positive results divided by the number of all positive results, including those not identified correctly, and the recall is the number of true positive results divided by the number of all samples that should have been identified as positive. Precision is also known as positive predictive value, and recall is also known as sensitivity in diagnostic binary classification.\n",
    "\n",
    "The F1 score is the harmonic mean of the precision and recall. The more generic {\\displaystyle F_{\\beta }}F_{\\beta } score applies additional weights, valuing one of precision or recall more than the other.\n",
    "\n",
    "The highest possible value of an F-score is 1.0, indicating perfect precision and recall, and the lowest possible value is 0, if either the precision or the recall is zero.\n",
    "\n",
    "iii) The Average Silhouette Width (ASW) is a popular cluster validation index to estimate the number of clusters. The question whether it also is suitable as a general objective function to be optimized for finding a clustering is addressed. Two algorithms (the standard version OSil and a fast version FOSil) are proposed, and they are compared with existing clustering methods in an extensive simulation study covering known and unknown numbers of clusters. Real data sets are analysed, partly exploring the use of the new methods with non-Euclidean distances. The ASW is shown to satisfy some axioms that have been proposed for cluster quality functions. The new methods prove useful and sensible in many cases, but some weaknesses are also highlighted. These also concern the use of the ASW for estimating the number of clusters together with other methods, which is of general interest due to the popularity of the ASW for this task.\n",
    "\n",
    "iv) A receiver operating characteristic curve, or ROC curve, is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied. The method was originally developed for operators of military radar receivers starting in 1941, which led to its name.\n",
    "\n",
    "The ROC curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings. The true-positive rate is also known as sensitivity, recall or probability of detection.[9] The false-positive rate is also known as probability of false alarm[9] and can be calculated as (1 − specificity). It can also be thought of as a plot of the power as a function of the Type I Error of the decision rule (when the performance is calculated from just a sample of the population, it can be thought of as estimators of these quantities). The ROC curve is thus the sensitivity or recall as a function of fall-out. In general, if the probability distributions for both detection and false alarm are known, the ROC curve can be generated by plotting the cumulative distribution function (area under the probability distribution from {\\displaystyle -\\infty }-\\infty  to the discrimination threshold) of the detection probability in the y-axis versus the cumulative distribution function of the false-alarm probability on the x-axis.\n",
    "\n",
    "ROC analysis provides tools to select possibly optimal models and to discard suboptimal ones independently from (and prior to specifying) the cost context or the class distribution. ROC analysis is related in a direct and natural way to cost/benefit analysis of diagnostic decision making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0f4567",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0baa7c0c",
   "metadata": {},
   "source": [
    "#Q1\n",
    "\n",
    "A feature is a measurable property of the object you’re trying to analyze. In datasets, features appear as columns.\n",
    "Features are the basic building blocks of datasets. The quality of the features in your dataset has a major impact on the quality of the insights you will gain when you use that dataset for machine learning. Additionally, different business problems within the same industry do not necessarily require the same features, which is why it is important to have a strong understanding of the business goals of your data science project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165d8f8a",
   "metadata": {},
   "source": [
    "#Q2\n",
    "\n",
    "The process of generating new variables (features) based on already existing variables is known as feature construction.\n",
    "\n",
    "Feature Construction is a useful process as it can add more information and give more insights of the data we are dealing with.\n",
    "\n",
    "Feature Construction is done by transforming the numerical features into categorical features which is done while performing Binning.\n",
    "\n",
    "Also, feature construction is done by decomposing variables so that these new variables can be used in various machine learning algorithms such as the creation of Dummy Variables by performing Encoding.\n",
    "\n",
    "Other ways of constructing include deriving features from the pre-existing features and coming up with more meaningful features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdc7192",
   "metadata": {},
   "source": [
    "#Q3\n",
    "\n",
    "When we have a feature where variables are just names and there is no order or rank to this variable's feature.\n",
    "\n",
    "\n",
    "For example: City of person lives in, Gender of person, Marital Status, etc…\n",
    "\n",
    "In the above example, We do not have any order or rank, or sequence. All the variables in the respective feature are equal. We can't give them any orders or ranks. Those features are called Nominal features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a2441d",
   "metadata": {},
   "source": [
    "#Q4\n",
    "\n",
    "We can also use LabelEncoder from sklearn library.\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "lbl = LabelEncoder()\n",
    "\n",
    " sample data\n",
    " \n",
    "df = pd.DataFrame({'V1': ['a','b','a','d'],\n",
    "                   'V2':['c','d','d','c']})\n",
    "\n",
    " apply function\n",
    " \n",
    "df.apply(lbl.fit_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5307102",
   "metadata": {},
   "source": [
    "#Q5\n",
    "\n",
    "In wrapper methods, the feature selection process is based on a specific machine learning algorithm that we are trying to fit on a given dataset.\n",
    "\n",
    "It follows a greedy search approach by evaluating all the possible combinations of features against the evaluation criterion. The evaluation criterion is simply the performance measure which depends on the type of problem, for e.g. For regression evaluation criterion can be p-values, R-squared, Adjusted R-squared, similarly for classification the evaluation criterion can be accuracy, precision, recall, f1-score, etc. Finally, it selects the combination of features that gives the optimal results for the specified machine learning algorithm.\n",
    "\n",
    "Advantages of wrapper method:\n",
    "    Less prone to local optima, interacts with the classifier, models feature dependencies, higher performance accuracy than filter.\n",
    "Disadvantages of wrapper method:\n",
    "    Computationally intensive, discriminative power, lower shortage training times, higher risk of over-fitting than deterministic algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3087f391",
   "metadata": {},
   "source": [
    "#Q6\n",
    "\n",
    "Feature selection technique is a knowledge discovery tool which provides an understanding of the problem through the analysis of the most relevant features. Feature selection aims at building better classifier by listing significant features which also helps in reducing computational overload. Due to existing high throughput technologies and their recent advancements are resulting in high dimensional data due to which feature selection is being treated as handy and mandatory in such datasets. This actually questions the interpretability and stability of traditional feature selection algorithms. The high correlation in features frequently produces multiple equally optimal signatures, which makes traditional feature selection method unstable and thus leading to instability which reduces the confidence of selected features. Stability is the robustness of the feature preferences it produces to perturbation of training samples. Stability indicates the reproducibility power of the feature selection method. High stability of the feature selection algorithm is equally important as the high classification accuracy when evaluating feature selection performance."
   ]
  },
  {
   "cell_type": "raw",
   "id": "091a26b9",
   "metadata": {},
   "source": [
    "#Q7\n",
    "\n",
    "Data redundancy occurs when the same piece of data is stored in two or more separate places and is a common occurrence in many businesses. As more companies are moving  away from siloed data to using a central repository to store information, they are finding that their database is filled with inconsistent duplicates of the same entry. Although it can be challenging to reconcile — or even benefit from — duplicate data entries, understanding how to reduce and track data redundancy efficiently can help mitigate long-term inconsistency issues for your business. \n",
    "Sometimes data redundancy happens by accident while other times it is intentional. Accidental data redundancy can be the result of a complex process or inefficient coding while intentional data redundancy can be used to protect data and ensure consistency — simply by leveraging the multiple occurrences of data for disaster recovery and quality checks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a971e1b9",
   "metadata": {},
   "source": [
    "#Q8\n",
    "\n",
    "Perhaps four of the most commonly used distance measures in machine learning are as follows: Hamming Distance. Euclidean Distance. Manhattan Distance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382e2fdf",
   "metadata": {},
   "source": [
    "#Q9\n",
    "\n",
    "Manhattan distance is usually preferred over the more common Euclidean distance when there is high dimensionality in the data. Hamming distance is used to measure the distance between categorical variables, and the Cosine distance metric is mainly used to find the amount of similarity between two data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4ee53a",
   "metadata": {},
   "source": [
    "#Q10\n",
    "\n",
    "The key difference between feature selection and feature extraction techniques used for dimensionality reduction is that while the original features are maintained in the case of feature selection algorithms, the feature extraction algorithms transform the data onto a new feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0c9ef9",
   "metadata": {},
   "source": [
    "#Q11\n",
    "\n",
    "ii) Feature Selection (FS) is a strategy that aims at making text document classifiers more efficient and accurate. However, when dealing with a new task, it is still difficult to quickly select a suitable one from various FS methods provided by many previous studies. Feature selection, as a preprocessing step to machine learning, has been very effective in reducing dimensionality, removing irrelevant data, and noise from data to improving result comprehensibility. Researchers have introduced many feature selection algorithms with different selection criteria. However, it has been discovered that no single criterion is best for all applications. We proposed a hybrid approach for feature selection called based on genetic algorithms (GAs) that employs a target learning algorithm to evaluate features, a wrapper method. The advantages of this approach include the ability to accommodate multiple feature selection criteria and find small subsets of features that perform well for the target algorithm. In this way, heterogeneous documents are summarized and presented in a uniform manner.\n",
    "\n",
    "iv)A receiver operating characteristic curve, or ROC curve, is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied. The method was originally developed for operators of military radar receivers starting in 1941, which led to its name.\n",
    "\n",
    "The ROC curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings. The true-positive rate is also known as sensitivity, recall or probability of detection. The false-positive rate is also known as probability of false alarm and can be calculated as (1 − specificity). It can also be thought of as a plot of the power as a function of the Type I Error of the decision rule (when the performance is calculated from just a sample of the population, it can be thought of as estimators of these quantities). The ROC curve is thus the sensitivity or recall as a function of fall-out. In general, if the probability distributions for both detection and false alarm are known, the ROC curve can be generated by plotting the cumulative distribution function (area under the probability distribution from {\\displaystyle -\\infty }-\\infty  to the discrimination threshold) of the detection probability in the y-axis versus the cumulative distribution function of the false-alarm probability on the x-axis.\n",
    "\n",
    "ROC analysis provides tools to select possibly optimal models and to discard suboptimal ones independently from (and prior to specifying) the cost context or the class distribution. ROC analysis is related in a direct and natural way to cost/benefit analysis of diagnostic decision making. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cb8a6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc2189e1",
   "metadata": {},
   "source": [
    "#Q1\n",
    "\n",
    "One Hot Encoding is a common way of preprocessing categorical features for machine learning models. This type of encoding creates a new binary feature for each possible category and assigns a value of 1 to the feature of each sample that corresponds to its original category."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f46e981",
   "metadata": {},
   "source": [
    "#Q2\n",
    "\n",
    "A bag-of-words is a representation of text that describes the occurrence of words within a document. It involves two things: A vocabulary of known words. A measure of the presence of known words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ae205f",
   "metadata": {},
   "source": [
    "#Q3\n",
    "\n",
    "A bag-of-n-grams model records the number of times that each n-gram appears in each document of a collection. An n-gram is a collection of n successive words. bagOfNgrams does not split text into words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665a028d",
   "metadata": {},
   "source": [
    "#Q4\n",
    "\n",
    "TF-IDF (term frequency-inverse document frequency) is a statistical measure that evaluates how relevant a word is to a document in a collection of documents.\n",
    "\n",
    "This is done by multiplying two metrics: how many times a word appears in a document, and the inverse document frequency of the word across a set of documents.\n",
    "\n",
    "It has many uses, most importantly in automated text analysis, and is very useful for scoring words in machine learning algorithms for Natural Language Processing (NLP)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adfb2cf",
   "metadata": {},
   "source": [
    "#Q5\n",
    "\n",
    "Out-of-vocabulary (OOV) are terms that are not part of the normal lexicon found in a natural language processing environment. In speech recognition, it's the audio signal that contains these terms. Word vectors are the mathematical equivalent of word meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17f25ab",
   "metadata": {},
   "source": [
    "#Q6\n",
    "\n",
    "It is an approach for representing words and documents. Word Embedding or Word Vector is a numeric vector input that represents a word in a lower-dimensional space. It allows words with similar meaning to have a similar representation. They can also approximate meaning. A word vector with 50 values can represent 50 unique features.\n",
    "\n",
    "Features: Anything that relates words to one another. Eg: Age, Sports, Fitness, Employed etc. Each word vector has values corresponding to these features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc3bb82",
   "metadata": {},
   "source": [
    "#Q7\n",
    "\n",
    "In the CBOW model, the distributed representations of context (or surrounding words) are combined to predict the word in the middle . While in the Skip-gram model, the distributed representation of the input word is used to predict the context ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a00755",
   "metadata": {},
   "source": [
    "#Q8\n",
    "\n",
    "Skip-gram is one of the unsupervised learning techniques used to find the most related words for a given word. Skip-gram is used to predict the context word for a given target word. It's reverse of CBOW algorithm. Here, target word is input while context words are output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a695c2",
   "metadata": {},
   "source": [
    "#Q9\n",
    "\n",
    "GloVe stands for global vectors for word representation. It is an unsupervised learning algorithm developed by Stanford for generating word embeddings by aggregating global word-word co-occurrence matrix from a corpus. The resulting embeddings show interesting linear substructures of the word in vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054a4bad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

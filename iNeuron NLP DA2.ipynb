{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82903d81",
   "metadata": {},
   "source": [
    "#Q1\n",
    "\n",
    "A corpus is a collection of authentic text or audio organized into datasets. Authentic here means text written or audio spoken by a native of the language or dialect. A corpus can be made up of everything from newspapers, novels, recipes, radio broadcasts to television shows, movies, and tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a947726",
   "metadata": {},
   "source": [
    "#Q2\n",
    "\n",
    "A token is an instance of a sequence of characters in some particular document that are grouped together as a useful semantic unit for processing. A type is the class of all tokens containing the same character sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cefddd",
   "metadata": {},
   "source": [
    "#Q3\n",
    "\n",
    "N-Grams are phrases cut out of a sentence with N cinsecutive words. Thus a Unigram takes a sentence and gives us all the words in that we fence. A Bigram takes a sentence and gives us sets of two consecutive words in the sentence. A Trigram gives sets of threee consecutive words in a sentence.\n",
    "\n",
    "Let me explain with an example.\n",
    "\n",
    "Unigram - [Let] [me] [explain] [with] [an] [example.]\n",
    "\n",
    "Bigram [let me] [me explain] [explain with] [with an] [an example]\n",
    "\n",
    "Trigram [let me explain] [me explain with] [explain with an] [with an example]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d98c040",
   "metadata": {},
   "source": [
    "#Q4\n",
    "\n",
    "There are two ways to generate N-grams, either by writing the logic yourself or by using the nltk library function.\n",
    "Logic:-\n",
    "    \n",
    "#Creating a function to generate N-Grams\n",
    "\n",
    "def generate_ngrams(text, WordsToCombine):\n",
    "\n",
    "words = text.split()\n",
    "\n",
    "output = []  \n",
    "\n",
    "for i in range(len(words)- WordsToCombine+1):\n",
    "\n",
    "output.append(words[i:i+WordsToCombine])\n",
    "\n",
    "return output\n",
    " \n",
    "#Calling the function\n",
    "\n",
    "generate_ngrams(text='this is a very good book to study', WordsToCombine=3)\n",
    "\n",
    "Library:-\n",
    "\n",
    "#NLTK function to generate ngrams\n",
    "\n",
    "import nltk\n",
    "\n",
    "from nltk.util import ngrams\n",
    " \n",
    "samplText='this is a very good book to study'\n",
    "\n",
    "NGRAMS=ngrams(sequence=nltk.word_tokenize(samplText), n=3)\n",
    "\n",
    "for grams in NGRAMS:\n",
    "\n",
    "print(grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4705e7b4",
   "metadata": {},
   "source": [
    "#Q5\n",
    "\n",
    "Lemmatisation (or lemmatization) in linguistics is the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d606ce",
   "metadata": {},
   "source": [
    "#Q6\n",
    "\n",
    "Stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma. Stemming is important in natural language understanding (NLU) and natural language processing (NLP)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7338051",
   "metadata": {},
   "source": [
    "#Q7\n",
    "\n",
    "It is a process of converting a sentence to forms â€“ list of words, list of tuples (where each tuple is having a form (word, tag)). The tag in case of is a part-of-speech tag, and signifies whether the word is a noun, adjective, verb, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0757c124",
   "metadata": {},
   "source": [
    "#Q8\n",
    "\n",
    "Shallow parsing (also chunking or light parsing) is an analysis of a sentence which first identifies constituent parts of sentences (nouns, verbs, adjectives, etc.) and then links them to higher order units that have discrete grammatical meanings (noun groups or phrases, verb groups, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f483f545",
   "metadata": {},
   "source": [
    "#Q9\n",
    "\n",
    "Chunking builds upon these grammatical parts to identify groups of words that go together to form symbolic meaning. This can be an adjective that goes along with a noun or a group of nouns related to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e47aabc",
   "metadata": {},
   "source": [
    "#Q10\n",
    "\n",
    "Named entity recognition (NER) is a sub-task of information extraction (IE) that seeks out and categorizes specified entities in a body or bodies of texts. NER is also known simply as entity identification, entity chunking and entity extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f04203e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

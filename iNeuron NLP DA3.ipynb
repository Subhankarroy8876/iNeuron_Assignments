{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0d964e3",
   "metadata": {},
   "source": [
    "#Q1\n",
    "\n",
    "RNN has a concept of “memory” which remembers all information about what has been calculated till time step t. RNNs are called recurrent because they perform the same task for every element of a sequence, with the output being depended on the previous computations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f6ad60",
   "metadata": {},
   "source": [
    "#Q2\n",
    "\n",
    "Backpropagation Through Time, or BPTT, is the application of the Backpropagation training algorithm to recurrent neural network applied to sequence data like a time series. A recurrent neural network is shown one input each timestep and predicts one output. Conceptually, BPTT works by unrolling all input timesteps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173cec8f",
   "metadata": {},
   "source": [
    "#Q3\n",
    "\n",
    "When there are more layers in the network, the value of the product of derivative decreases until at some point the partial derivative of the loss function approaches a value close to zero, and the partial derivative vanishes. We call this the vanishing gradient problem.\n",
    "Exploding gradients are a problem when large error gradients accumulate and result in very large updates to neural network model weights during training. Gradients are used during training to update the network weights, but when the typically this process works best when these updates are small and controlled."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3395e060",
   "metadata": {},
   "source": [
    "#Q4\n",
    "\n",
    "Long short-term memory (LSTM) is an artificial neural network used in the fields of artificial intelligence and deep learning. Unlike standard feedforward neural networks, LSTM has feedback connections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a312a53",
   "metadata": {},
   "source": [
    "#Q5\n",
    "\n",
    "The Gated Recurrent Unit (GRU) is a type of Recurrent Neural Network (RNN) that, in certain cases, has advantages over long short term memory (LSTM). GRU uses less memory and is faster than LSTM, however, LSTM is more accurate when using datasets with longer sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b753fa04",
   "metadata": {},
   "source": [
    "#Q6\n",
    "\n",
    "The architecture of peephole LSTM demonstrated the LSTM's superiority over other RNN in presence of significant time lags between relevant events but did not require the network to extract relevant information conveyed by the size of the time lags. Here, we show that LSTM also can solve such tasks, by in a highly nonlinear fashion way. We extend LSTM cells by peephole connections that allow them to inspect their current internal states. It is remarkable that LSTM can learn precise and stable timing algorithms without any forcing and despite the very uninformative target signals which changed rarely. This makes LSTM a promising method for many data processing needs to be measured."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55c4845",
   "metadata": {},
   "source": [
    "#Q7\n",
    "\n",
    "A Bidirectional RNN is a combination of two RNNs training the network in opposite directions, one from the beginning to the end of a sequence, and the other, from the end to the beginning of a sequence. It helps in analyzing the future events by not limiting the model's learning to past and present."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd33df35",
   "metadata": {},
   "source": [
    "#Q9\n",
    "\n",
    "Bidirectional LSTM (BiLSTM) is a recurrent neural network used primarily on natural language processing. Unlike standard LSTM, the input flows in both directions, and it's capable of utilizing information from both sides."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea79aa30",
   "metadata": {},
   "source": [
    "#Q10\n",
    "\n",
    "A Bidirectional GRU, or BiGRU, is a sequence processing model that consists of two GRUs. one taking the input in a forward direction, and the other in a backwards direction. It is a bidirectional recurrent neural network with only the input and forget gates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a056ea05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4540972d",
   "metadata": {},
   "source": [
    "#Q1\n",
    "\n",
    "In Sequence to Sequence Learning, RNN is trained to map an input sequence to an output sequence which is not necessarily of the same length. Applications are speech recognition, machine translation, image captioning and question answering.\n",
    "A variable-length context vector can be used instead of a Ô¨Åxed-size vector. An Attention mechanism can be used to produces a sequence of vectors from the encoder RNN from each time step of the input sequence. The Decoder learns to pay selective attention to the vectors to produce the output at each time step.\n",
    "he RNN model takes a single vector as input and produces a sequence as output. An example of these models can be image to sentence model, which takes an image(consider it as a vector) and then produces a sentence to describe that image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3559e429",
   "metadata": {},
   "source": [
    "#Q2\n",
    "\n",
    "seq-2-seq RNNs translate one word at a time\n",
    "\n",
    "encoder-decoder RNNs read & translate a sentence at a time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4b70de",
   "metadata": {},
   "source": [
    "#Q3\n",
    "\n",
    "1. Run a frame from each second of video through a CNN\n",
    "\n",
    "2. Feed CNN outputs as input sequence to RNN\n",
    "\n",
    "3. Feed RNN outputs to softmax layer for probabilities of each class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b43fc24",
   "metadata": {},
   "source": [
    "#Q4\n",
    "\n",
    "1. avoids out-of-memory errors\n",
    "\n",
    "2. directly takes single tensor as input and output (covering all time steps)\n",
    "\n",
    "3. no need to stack, unstack, or transpose\n",
    "\n",
    "4. generates a smaller easier to visualize graph in TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e347d78",
   "metadata": {},
   "source": [
    "#Q5\n",
    "\n",
    "1. set sequence_length parameter when calling static_rnn() or dynamic_rnn()\n",
    "\n",
    "2. pad smaller input/output to make them same size as largest input/output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fa848c",
   "metadata": {},
   "source": [
    "#Q6\n",
    "\n",
    "place each layer on a different GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f09f25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

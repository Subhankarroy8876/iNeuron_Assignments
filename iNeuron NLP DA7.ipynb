{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d2e64bb",
   "metadata": {},
   "source": [
    "#Q1\n",
    "\n",
    "BERT is basically an Encoder stack of transformer architecture. A transformer architecture is an encoder-decoder network that uses self-attention on the encoder side and attention on the decoder side. BERTBASE has 12 layers in the Encoder stack while BERTLARGE has 24 layers in the Encoder stack."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c90ea1a",
   "metadata": {},
   "source": [
    "#Q2\n",
    "\n",
    "Masked language modeling (MLM), a self-supervised pretraining objective, is widely used in natural language processing for learning text representations. MLM trains a model to predict a random sample of input tokens that have been replaced by a [MASK] placeholder in a multi-class setting over the entire vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb4d861",
   "metadata": {},
   "source": [
    "#Q3\n",
    "\n",
    "Next sentence prediction (NSP) is one-half of the training process behind the BERT model (the other being masked-language modeling — MLM). Where MLM teaches BERT to understand relationships between words — NSP teaches BERT to understand longer-term dependencies across sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f831638",
   "metadata": {},
   "source": [
    "#Q4\n",
    "\n",
    "Matthew's correlation coefficient, also abbreviated as MCC was invented by Brian Matthews in 1975. MCC is a statistical tool used for model evaluation. Its job is to gauge or measure the difference between the predicted values and actual values and is equivalent to chi-square statistics for a 2 x 2 contingency table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e4dee5",
   "metadata": {},
   "source": [
    "#Q5\n",
    "\n",
    "The Matthews correlation coefficient (MCC), instead, is a more reliable statistical rate which produces a high score only if the prediction obtained good results in all of the four confusion matrix categories (true positives, false negatives, true negatives, and false positives), proportionally both to the size of positive elements and the size of negative elements in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5bd8cf",
   "metadata": {},
   "source": [
    "#Q6\n",
    "\n",
    "In natural language processing, semantic role labeling (also called shallow semantic parsing or slot-filling) is the process that assigns labels to words or phrases in a sentence that indicates their semantic role in the sentence, such as that of an agent, goal, or result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807d9686",
   "metadata": {},
   "source": [
    "#Q8\n",
    "\n",
    "Recognizing Textual Entailment (RTE) was proposed as a unified evaluation framework to compare semantic understanding of different NLP systems. In this survey paper, we provide an overview of different approaches for evaluating and understanding the reasoning capabilities of NLP systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457bbfe1",
   "metadata": {},
   "source": [
    "#Q9\n",
    "\n",
    "GPT model was based on Transformer architecture. It was made of decoders stacked on top of each other (12 decoders). These models were same as BERT as they were also based on Transformer architecture. The difference in architecture with BERT is that it used stacked encoder layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c76987",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
